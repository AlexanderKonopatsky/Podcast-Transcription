# Как работает приложение

## Обзор

Приложение выполняет автоматическую транскрибацию подкастов с определением спикеров (диаризацией). На выходе получается текст с указанием, кто из участников что говорит и в какое время.

## Основные компоненты

### 1. Диаризация спикеров (pyannote)

**Модель:** `pyannote/speaker-diarization-3.1`

Диаризация отвечает на вопрос: "Кто говорит в каждый момент времени?"

**Этапы диаризации:**
1. **Сегментация** — разбивка аудио на фрагменты по 10 секунд с шагом 2.5 сек
2. **Извлечение эмбеддингов** — преобразование каждого фрагмента в вектор (характеристика голоса)
3. **Кластеризация** — группировка похожих голосов в спикеров
4. **Финализация** — присвоение меток спикеров (SPEAKER_00, SPEAKER_01, ...)

**Результат:** список временных сегментов с указанием спикера
```
[0.5 - 3.2] SPEAKER_00
[3.2 - 8.1] SPEAKER_01
[8.1 - 15.0] SPEAKER_00
```

### 2. Транскрибация (faster-whisper)

**Модель:** OpenAI Whisper (large-v3 по умолчанию)

Транскрибация отвечает на вопрос: "Что говорится?"

**Особенности:**
- Использует `faster-whisper` — оптимизированную версию на базе CTranslate2
- Поддержка GPU через CUDA (float16) или CPU (int8)
- VAD фильтрация — пропуск тишины для ускорения
- Word timestamps — точные временные метки для каждого слова

**Результат:** список сегментов текста с временными метками
```
[0.5 - 2.8] "Добро пожаловать в наш подкаст"
[3.0 - 5.5] "Спасибо что пригласили"
```

### 3. Объединение результатов

Финальный этап — привязка спикеров к тексту:

1. Для каждого текстового сегмента вычисляется его середина
2. Ищется диаризационный сегмент, который содержит эту точку
3. Спикер из диаризации присваивается текстовому сегменту

```
Текст: [0.5 - 2.8] "Добро пожаловать..." → середина = 1.65
Диаризация: [0.5 - 3.2] SPEAKER_00 ← содержит 1.65
Результат: SPEAKER_00: "Добро пожаловать..."
```

## Поток данных

```
┌─────────────┐
│ Аудиофайл   │
│ (MP3/WAV)   │
└──────┬──────┘
       │
       ▼
┌──────────────────────────────────────────────────────┐
│                    Параллельная обработка             │
│  ┌────────────────────┐    ┌────────────────────┐    │
│  │    Диаризация      │    │   Транскрибация    │    │
│  │    (pyannote)      │    │  (faster-whisper)  │    │
│  │                    │    │                    │    │
│  │ WHO говорит        │    │ WHAT говорится     │    │
│  │ [start, end,       │    │ [start, end,       │    │
│  │  speaker]          │    │  text, words]      │    │
│  └─────────┬──────────┘    └─────────┬──────────┘    │
│            │                         │               │
│            └───────────┬─────────────┘               │
│                        │                             │
│                        ▼                             │
│              ┌─────────────────┐                     │
│              │   Объединение   │                     │
│              │   результатов   │                     │
│              └────────┬────────┘                     │
└───────────────────────┼──────────────────────────────┘
                        │
                        ▼
           ┌────────────────────────┐
           │  Результат с спикерами │
           │  [speaker, text, time] │
           └────────────┬───────────┘
                        │
         ┌──────────────┼──────────────┐
         ▼              ▼              ▼
    ┌─────────┐   ┌──────────┐   ┌─────────┐
    │  .txt   │   │  .json   │   │  .srt   │
    │ Текст   │   │ Данные   │   │ Субтитры│
    └─────────┘   └──────────┘   └─────────┘
```

## Использование GPU

### CUDA / GPU ускорение

Приложение автоматически использует GPU если доступен:

| Компонент | GPU режим | CPU режим |
|-----------|-----------|-----------|
| pyannote (диаризация) | CUDA tensors | CPU tensors |
| faster-whisper | float16 CUDA | int8 CPU |
| Загрузка аудио | GPU tensors | CPU tensors |

### Оптимизации для памяти

1. **Последовательная обработка** — диаризация освобождает VRAM перед транскрибацией
2. **torch.cuda.empty_cache()** — явная очистка GPU памяти между этапами
3. **Batch размер эмбеддингов** — увеличен до 32 для эффективного использования GPU

## Совместимость

### Патчи для PyTorch 2.6+

```python
# PyTorch 2.6+ по умолчанию использует weights_only=True
# pyannote модели требуют старый режим
torch.load = patched_version(weights_only=False)
```

### Патчи для torchaudio nightly

Некоторые функции были удалены в nightly версиях:
- `torchaudio.info()` — заменено на soundfile
- `torchaudio.load()` — заменено на soundfile + GPU перенос
- `torchaudio.AudioMetaData` — создан dataclass-заглушка

## Форматы вывода

### TXT — читаемый текст
```
[00:15] SPEAKER_00:
  Добро пожаловать в наш подкаст!

[00:20] SPEAKER_01:
  Спасибо, что пригласили.
```

### JSON — полные данные
```json
{
  "segments": [
    {
      "start": 0.5,
      "end": 2.8,
      "text": "Добро пожаловать...",
      "speaker": "SPEAKER_00",
      "words": [{"start": 0.5, "end": 0.8, "word": "Добро"}, ...]
    }
  ],
  "language": "ru",
  "duration": 3600.0,
  "num_speakers": 3
}
```

### SRT — субтитры
```
1
00:00:00,500 --> 00:00:02,800
[SPEAKER_00] Добро пожаловать в наш подкаст!

2
00:00:03,000 --> 00:00:05,500
[SPEAKER_01] Спасибо, что пригласили.
```

## Идентификация спикеров по голосу

### Как это работает

1. **Создание профилей** — пользователь загружает образцы голосов (MP3 файлы) в папку `speaker_samples/`
2. **Извлечение эмбеддингов** — модель `pyannote/embedding` преобразует голос в вектор из 512 чисел
3. **Сохранение профилей** — эмбеддинги сохраняются в `speaker_profiles.json`
4. **Сопоставление** — при транскрибации эмбеддинги спикеров сравниваются с профилями через cosine distance

### Speaker Embedding

**Эмбеддинг голоса** — это вектор из 512 чисел, который уникально характеризует голос человека независимо от того, что он говорит.

```
Образец голоса "Зенур.mp3" → [0.123, -0.456, 0.789, ...] (512 чисел)
```

### Сопоставление (Cosine Distance)

При транскрибации pyannote возвращает центроиды (усреднённые эмбеддинги) для каждого спикера. Мы сравниваем их с профилями:

```
SPEAKER_00 embedding → сравниваем с профилями → cosine distance
  - Зенур:  0.23 (близко!)
  - Серега: 0.67 (далеко)
  → SPEAKER_00 = Зенур
```

**Порог (threshold):** если расстояние < 0.5, считаем что это один человек.

### Формат профилей (speaker_profiles.json)

```json
{
  "version": "1.0",
  "embedding_model": "pyannote/embedding",
  "profiles": {
    "Зенур": {
      "centroid": [0.123, -0.456, ...],
      "samples": ["speaker_samples/Зенур.mp3"],
      "created_at": "2026-01-24T12:00:00"
    }
  }
}
```

## Требования

- **Python 3.10+**
- **PyTorch** с CUDA (опционально для GPU)
- **HuggingFace токен** — для скачивания моделей pyannote
- **ffprobe** — для определения длительности аудио

## Ограничения

1. **Количество спикеров** — нужно указывать заранее (--speakers N)
2. **Качество диаризации** — зависит от качества аудио и различимости голосов
3. **VRAM** — large-v3 модель требует ~4-6 GB GPU памяти
4. **Идентификация спикеров** — требует предварительно загруженные образцы голосов
