"""
Sentiment analyzer for extracting opinions from podcast transcripts.
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime, date
import re
import asyncio

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag.search import search
from llm.client import chat, achat
from config import SENTIMENT_MAX_TEXT_LENGTH


class SentimentAnalyzer:
    """Analyze sentiment and track opinion changes over time."""

    def __init__(self):
        """Initialize sentiment analyzer."""
        pass

    def analyze_entity(
        self,
        entity: str,
        speaker: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        top_k: int = 50
    ) -> List[Dict]:
        """
        Analyze sentiment about an entity across podcasts.

        Args:
            entity: Topic to analyze (e.g., "Ğ±Ğ¸Ñ‚ĞºĞ¾Ğ¸Ğ½", "eigen layer")
            speaker: Optional speaker filter
            start_date: Optional start date filter (YYYY-MM-DD)
            end_date: Optional end date filter (YYYY-MM-DD)
            top_k: Number of chunks to analyze

        Returns:
            List of sentiment results with dates, scores, quotes
        """
        # Build search query
        search_query = f"{entity}"

        # Set time filter
        time_filter = "historical"  # Get all history
        if start_date and end_date:
            time_filter = {"start": start_date, "end": end_date}

        # Search for mentions
        print(f"ğŸ” ĞŸĞ¾Ğ¸ÑĞº ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹ '{entity}'...")
        try:
            results = search(
                search_query,
                speaker=speaker,
                top_k=top_k,
                time_filter=time_filter
            )
        except Exception as e:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
            return []

        if not results:
            print(f"Ğ£Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ '{entity}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
            return []

        print(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(results)} ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹")

        # Group by podcast/date
        grouped = self._group_by_podcast(results)
        print(f"Ğ’ {len(grouped)} Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°Ñ…")

        # Analyze each group
        sentiments = []
        for podcast_info, result_list in grouped.items():
            podcast_date, podcast_speaker = podcast_info

            # Combine text from chunks
            combined_text = "\n\n".join([
                result['chunk'].get('text', '')
                for result in result_list
            ])

            # Analyze sentiment via LLM
            sentiment = self._analyze_sentiment_llm(
                entity=entity,
                text=combined_text,
                speaker=podcast_speaker,
                date=podcast_date
            )

            if sentiment:
                sentiments.append(sentiment)

        # Sort by date
        sentiments.sort(key=lambda x: x['date'])

        return sentiments

    async def analyze_entity_async(
        self,
        entity: str,
        speaker: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        top_k: int = 500,
        max_concurrent: int = 10
    ) -> List[Dict]:
        """
        Async version: Analyze sentiment with parallel LLM requests.

        Args:
            entity: Topic to analyze (e.g., "Ğ±Ğ¸Ñ‚ĞºĞ¾Ğ¸Ğ½", "eigen layer")
            speaker: Optional speaker filter
            start_date: Optional start date filter (YYYY-MM-DD)
            end_date: Optional end date filter (YYYY-MM-DD)
            top_k: Number of chunks to analyze
            max_concurrent: Max concurrent LLM requests (rate limiting)

        Returns:
            List of sentiment results with dates, scores, quotes, and combined_text
        """
        # Build search query
        search_query = f"{entity}"

        # Set time filter
        time_filter = "historical"  # Get all history
        if start_date and end_date:
            time_filter = {"start": start_date, "end": end_date}

        # Search for mentions
        print(f"ğŸ” ĞŸĞ¾Ğ¸ÑĞº ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹ '{entity}'...")
        try:
            results = search(
                search_query,
                speaker=speaker,
                top_k=top_k,
                time_filter=time_filter
            )
        except Exception as e:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
            return []

        if not results:
            print(f"Ğ£Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ '{entity}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
            return []

        print(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(results)} ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹")

        # Group by podcast/date
        grouped = self._group_by_podcast(results)
        print(f"Ğ’ {len(grouped)} Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°Ñ…")

        # Create semaphore for rate limiting
        semaphore = asyncio.Semaphore(max_concurrent)

        # Prepare tasks for parallel execution
        tasks = []
        for podcast_info, result_list in grouped.items():
            podcast_date, podcast_speaker = podcast_info

            # Combine text from chunks
            combined_text = "\n\n".join([
                result['chunk'].get('text', '')
                for result in result_list
            ])

            # Create async task
            task = self._analyze_sentiment_llm_async(
                entity=entity,
                text=combined_text,
                speaker=podcast_speaker,
                date=podcast_date,
                semaphore=semaphore
            )
            tasks.append((task, combined_text, podcast_info))

        # Execute all in parallel with rate limiting
        print(f"âš¡ ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ {len(tasks)} Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ¾Ğ² (Ğ¼Ğ°ĞºÑ. {max_concurrent} Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾)...")

        results = await asyncio.gather(*[t[0] for t in tasks], return_exceptions=True)

        # Process results
        sentiments = []
        success_count = 0
        error_count = 0

        for (task, combined_text, podcast_info), result in zip(tasks, results):
            if isinstance(result, Exception):
                error_count += 1
                print(f"[ĞĞ¨Ğ˜Ğ‘ĞšĞ] {podcast_info[0]} - {podcast_info[1]}: {type(result).__name__}: {result}")
                continue

            if result:
                # Add original text for TXT export
                result['combined_text'] = combined_text
                sentiments.append(result)
                success_count += 1

        print(f"âœ… ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½: {success_count} ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾, {error_count} Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº")

        if error_count > 0 and error_count == len(tasks):
            raise Exception("Ğ’ÑĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ¸ÑÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº OpenRouter.")

        # Sort by date
        sentiments.sort(key=lambda x: x['date'])

        return sentiments

    def _group_by_podcast(self, results: List[Dict]) -> Dict[tuple, List[Dict]]:
        """Group search results by podcast (date + speaker)."""
        grouped = {}

        for result in results:
            # Result structure: {"chunk": {...}, "score": ..., "podcast_date": ...}
            chunk = result.get('chunk', {})
            podcast_date = str(result.get('podcast_date', 'unknown'))

            # Get speakers from chunk
            speakers = chunk.get('speakers', ['Unknown'])

            # Group by date and each speaker
            for speaker in speakers:
                key = (podcast_date, speaker)
                if key not in grouped:
                    grouped[key] = []
                grouped[key].append(result)

        return grouped

    def _analyze_sentiment_llm(
        self,
        entity: str,
        text: str,
        speaker: str,
        date: str
    ) -> Optional[Dict]:
        """
        Analyze sentiment using LLM.

        Returns:
            Dict with sentiment data or None if failed
        """
        system_prompt = """Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….
Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° - Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ° Ğº Ñ‚ĞµĞ¼Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°."""

        user_prompt = f"""ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ° Ğº Ñ‚ĞµĞ¼Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°.

Ğ¢ĞµĞ¼Ğ°: {entity}
Ğ¡Ğ¿Ğ¸ĞºĞµÑ€: {speaker}
Ğ”Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°: {date}

Ğ¦Ğ¸Ñ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°:
{text[:SENTIMENT_MAX_TEXT_LENGTH]}

ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸:
1. **Sentiment** (Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ·): "Ğ±Ñ‹Ñ‡Ğ¸Ğ¹" (bullish), "Ğ¼ĞµĞ´Ğ²ĞµĞ¶Ğ¸Ğ¹" (bearish), "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹" (neutral)
2. **Score** (Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ñ‚ -1 Ğ´Ğ¾ +1):
   - -1.0 Ğ´Ğ¾ -0.6: Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾
   - -0.6 Ğ´Ğ¾ -0.2: Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾
   - -0.2 Ğ´Ğ¾ +0.2: Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾
   - +0.2 Ğ´Ğ¾ +0.6: Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾
   - +0.6 Ğ´Ğ¾ +1.0: Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾
3. **Reasoning**: ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
4. **Key_quote**: ÑĞ°Ğ¼Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ° (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)

Ğ’ĞĞ–ĞĞ: ĞÑ‚Ğ²ĞµÑ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¼ JSON, Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°:
{{"sentiment": "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹", "score": 0.0, "reasoning": "...", "key_quote": "..."}}"""

        try:
            response = chat(system_prompt, user_prompt, model="google/gemini-2.5-flash-lite")

            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                response = json_match.group(0)

            # Parse JSON
            sentiment_data = json.loads(response)

            # Add metadata
            sentiment_data['date'] = date
            sentiment_data['speaker'] = speaker
            sentiment_data['entity'] = entity

            return sentiment_data

        except json.JSONDecodeError as e:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° JSON Ğ´Ğ»Ñ {date}: {e}")
            print(f"Response: {response[:200]}")
            return None
        except Exception as e:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ {date}: {e}")
            return None

    async def _analyze_sentiment_llm_async(
        self,
        entity: str,
        text: str,
        speaker: str,
        date: str,
        semaphore: asyncio.Semaphore
    ) -> Optional[Dict]:
        """
        Async version: Analyze sentiment using LLM with rate limiting.

        Args:
            entity: Topic being analyzed
            text: Combined text from podcast chunks
            speaker: Speaker name
            date: Podcast date
            semaphore: Asyncio semaphore for rate limiting

        Returns:
            Dict with sentiment data or None if failed
        """
        async with semaphore:  # Acquire slot (max N concurrent)
            system_prompt = """Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….
Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° - Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ° Ğº Ñ‚ĞµĞ¼Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°."""

            user_prompt = f"""ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ° Ğº Ñ‚ĞµĞ¼Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°.

Ğ¢ĞµĞ¼Ğ°: {entity}
Ğ¡Ğ¿Ğ¸ĞºĞµÑ€: {speaker}
Ğ”Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°: {date}

Ğ¦Ğ¸Ñ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ°:
{text[:SENTIMENT_MAX_TEXT_LENGTH]}

ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸:
1. **Sentiment** (Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ·): "Ğ±Ñ‹Ñ‡Ğ¸Ğ¹" (bullish), "Ğ¼ĞµĞ´Ğ²ĞµĞ¶Ğ¸Ğ¹" (bearish), "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹" (neutral)
2. **Score** (Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ñ‚ -1 Ğ´Ğ¾ +1):
   - -1.0 Ğ´Ğ¾ -0.6: Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾
   - -0.6 Ğ´Ğ¾ -0.2: Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾
   - -0.2 Ğ´Ğ¾ +0.2: Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾
   - +0.2 Ğ´Ğ¾ +0.6: Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾
   - +0.6 Ğ´Ğ¾ +1.0: Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾
3. **Reasoning**: ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
4. **Key_quote**: ÑĞ°Ğ¼Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ° (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)

Ğ’ĞĞ–ĞĞ: ĞÑ‚Ğ²ĞµÑ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¼ JSON, Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°:
{{"sentiment": "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹", "score": 0.0, "reasoning": "...", "key_quote": "..."}}"""

            try:
                response = await achat(
                    system_prompt,
                    user_prompt,
                    model="google/gemini-2.5-flash-lite",
                    timeout=90.0
                )

                # Extract JSON from response
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    response = json_match.group(0)

                # Parse JSON
                sentiment_data = json.loads(response)

                # Add metadata
                sentiment_data['date'] = date
                sentiment_data['speaker'] = speaker
                sentiment_data['entity'] = entity

                return sentiment_data

            except json.JSONDecodeError as e:
                print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° JSON Ğ´Ğ»Ñ {date}: {e}")
                return None
            except Exception as e:
                print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ {date}: {e}")
                return None

    def summarize_trend(self, sentiments: List[Dict]) -> str:
        """
        Generate a text summary of sentiment trend in HTML format.

        Args:
            sentiments: List of sentiment results

        Returns:
            Human-readable summary in HTML
        """
        if not sentiments:
            return "ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."

        entity = sentiments[0]['entity']

        # Group by speaker
        by_speaker = {}
        for s in sentiments:
            speaker = s['speaker']
            if speaker not in by_speaker:
                by_speaker[speaker] = []
            by_speaker[speaker].append(s)

        summary_lines = [f"ğŸ“Š <b>ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¾ '{self._escape_html(entity)}':</b>\n"]

        for speaker, speaker_sentiments in by_speaker.items():
            summary_lines.append(f"\n<b>{self._escape_html(speaker)}:</b>")

            # Show ALL entries (no limit)
            for s in speaker_sentiments:
                score = s['score']
                emoji = self._score_to_emoji(score)
                summary_lines.append(
                    f"  {s['date']}: {emoji} {s['sentiment']} ({score:+.2f})"
                )
                if s.get('key_quote'):
                    quote = s['key_quote'][:80]
                    summary_lines.append(f"    ğŸ’¬ <i>{self._escape_html(quote)}</i>")

            # Calculate trend
            if len(speaker_sentiments) >= 2:
                first_score = speaker_sentiments[0]['score']
                last_score = speaker_sentiments[-1]['score']
                trend_change = last_score - first_score

                if trend_change > 0.2:
                    trend = "ğŸ“ˆ Ğ¢Ñ€ĞµĞ½Ğ´: ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼"
                elif trend_change < -0.2:
                    trend = "ğŸ“‰ Ğ¢Ñ€ĞµĞ½Ğ´: ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼"
                else:
                    trend = "ğŸ“Š Ğ¢Ñ€ĞµĞ½Ğ´: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ"

                summary_lines.append(f"  {trend} (Î”{trend_change:+.2f})")

        # Return full text (no truncation)
        return "\n".join(summary_lines)

    def _escape_html(self, text: str) -> str:
        """Escape HTML special characters."""
        if not text:
            return ""
        return (text
                .replace("&", "&amp;")
                .replace("<", "&lt;")
                .replace(">", "&gt;")
                .replace('"', "&quot;"))

    def _score_to_emoji(self, score: float) -> str:
        """Convert sentiment score to emoji."""
        if score > 0.6:
            return "ğŸš€"
        elif score > 0.2:
            return "ğŸ“ˆ"
        elif score > -0.2:
            return "â¡ï¸"
        elif score > -0.6:
            return "ğŸ“‰"
        else:
            return "ğŸ»"
